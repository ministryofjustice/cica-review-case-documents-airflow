{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcdbd3ce",
   "metadata": {},
   "source": [
    "Imports and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eaa0770",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from opensearchpy import OpenSearch\n",
    "\n",
    "# --- Configuration ---\n",
    "load_dotenv()\n",
    "DOMAIN_NAME = os.getenv(\"DOMAIN_NAME\")\n",
    "AWS_REGION = os.getenv(\"AWS_REGION\")\n",
    "OPENSEARCH_PORT = os.getenv(\"OPENSEARCH_PORT\")\n",
    "INDEX_NAME = os.getenv(\"INDEX_NAME\")\n",
    "OPENSEARCH_HOST = f\"{DOMAIN_NAME}.{AWS_REGION}.opensearch.localhost.localstack.cloud\"\n",
    "OPENSEARCH_USERNAME = os.getenv(\"OPENSEARCH_USERNAME\")\n",
    "OPENSEARCH_PASSWORD = os.getenv(\"OPENSEARCH_PASSWORD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eceabcae",
   "metadata": {},
   "source": [
    "OpenSearch Client Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c300ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a local OpenSearch instance, typically you'd use HTTP and basic auth.\n",
    "# If you are using AWS OpenSearch Service, you would need AWS4Auth.\n",
    "# For simplicity with a local docker-compose setup, we'll use basic auth.\n",
    "auth = (OPENSEARCH_USERNAME, OPENSEARCH_PASSWORD)\n",
    "client = OpenSearch(\n",
    "    hosts=[{\"host\": OPENSEARCH_HOST, \"port\": OPENSEARCH_PORT}],\n",
    "    http_auth=auth,\n",
    "    http_compress=True,  # enables gzip compression for request bodies\n",
    "    use_ssl=True,  # Use SSL for local OpenSearch, usually self-signed\n",
    "    verify_certs=False,  # Do not verify certs for local self-signed certs\n",
    "    ssl_assert_hostname=False,\n",
    "    ssl_show_warn=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0e6304",
   "metadata": {},
   "source": [
    "Extract text from PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "be53363d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from document_search_airflow.extractor_pdf_with_text import extract_text_from_pdf\n",
    "\n",
    "pdf_filename = \"ai-04-00049.pdf\"\n",
    "pages = extract_text_from_pdf(pdf_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471ac427",
   "metadata": {},
   "source": [
    "Chunk text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5a49e8b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.57it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.71it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 34.32it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 42.57it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 42.67it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 38.41it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 37.31it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 40.29it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.43it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 25.35it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 48.93it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 41.39it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 39.28it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 37.68it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 38.60it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 33.11it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 38.13it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 37.71it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 42.75it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 49.20it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 41.84it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 46.16it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 42.18it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 40.53it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.74it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.62it/s]\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import SentenceTransformersTokenTextSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "splitter = SentenceTransformersTokenTextSplitter(\n",
    "    model_name=\"sentence-transformers/all-mpnet-base-v2\",  # characters or ~512 tokens\n",
    "    chunk_overlap=50,\n",
    "    tokens_per_chunk=256,\n",
    ")\n",
    "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")  # 384‑D\n",
    "\n",
    "\n",
    "extracted_data = []\n",
    "for page_number, page_text in pages:\n",
    "    chunks = splitter.split_text(page_text)\n",
    "    embeddings = model.encode(chunks, batch_size=32, show_progress_bar=True)\n",
    "    extracted_data.append((page_number, page_text, chunks, embeddings))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aae6eba",
   "metadata": {},
   "source": [
    "Embed chunks using local embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4ffaed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.42s/it]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")  # 384‑D\n",
    "embeddings = model.encode(chunks, batch_size=32, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073cba12",
   "metadata": {},
   "source": [
    "Insert into OpenSearch Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f799f1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from opensearchpy import OpenSearch, helpers\n",
    "\n",
    "bulk_actions = (\n",
    "    {\n",
    "        \"_index\": \"rag_docs\",\n",
    "        \"_id\": f\"{file_name}_{page}_{i}\",\n",
    "        \"_source\": {\n",
    "            \"chunk_text\": chunk,\n",
    "            \"embedding\": emb.tolist(),  # ndarray → list\n",
    "            \"file_name\": file_name,\n",
    "            \"page\": page,\n",
    "            \"chunk_index\": i,\n",
    "            \"source_stage\": \"pdf_text\",\n",
    "        },\n",
    "    }\n",
    "    for i, (chunk, emb) in enumerate(zip(chunks, embeddings))\n",
    ")\n",
    "\n",
    "helpers.bulk(client, bulk_actions, request_timeout=120)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84dbb113",
   "metadata": {},
   "outputs": [],
   "source": [
    "from opensearchpy import helpers\n",
    "\n",
    "bulk_actions = []\n",
    "for page_number, page_text, chunks, embeddings in extracted_data:\n",
    "    for chunk_index, (chunk_text, embedding) in enumerate(zip(chunks, embeddings)):\n",
    "        doc = {\n",
    "            \"_index\": \"rag_docs\",\n",
    "            \"_id\": f\"{file_hash}_{page_number}_{chunk_index}\",\n",
    "            \"_source\": {\n",
    "                \"file_name\": file_name,\n",
    "                \"page\": page_number,\n",
    "                \"chunk_index\": chunk_index,\n",
    "                \"chunk_text\": chunk_text,\n",
    "                \"embedding\": embedding,\n",
    "            },\n",
    "        }\n",
    "        bulk_actions.append(doc)\n",
    "helpers.bulk(client, bulk_actions, request_timeout=120)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pdf-with-text",
   "language": "python",
   "name": "pdf-with-text"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
