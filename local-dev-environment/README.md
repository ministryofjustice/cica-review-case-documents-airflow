# Localstack local developer environment

Docker and LocalStack resources to be created:

- [Localstack](https://www.localstack.cloud/)
- [OpenSearch](https://docs.localstack.cloud/aws/services/opensearch/) 
- [OpenSearch Dashboard](https://docs.opensearch.org/docs/latest/dashboards/)
- OpenSearch Domain:```case-document-search-domain```
- Opensearch Index:```case-documents```, [setup-opensearch.sh](./init-scripts/create-opensearch-resources.sh)
- AWS resources, [setup-aws-resources.sh](./init-scripts/create-aws-resources.sh)

## Pre-requisites

- [Docker](https://docs.docker.com/get-started/get-docker/)
- [Docker Desktop](https://docs.docker.com/desktop/)
- [LocalStack Desktop](https://docs.localstack.cloud/aws/capabilities/web-app/localstack-desktop/)
- **For VPN WSL users**: If you are running the local environment from behind a corporate VPN with SSL inspection, you must also set the following environment variables in your .bashrc to allow LocalStack to trust your custom certificates:
```
# Ensures LocalStack and its internal services trust the custom CA
export LOCALSTACK_REQUESTS_CA_BUNDLE="/home/your_user/custom_ca_bundle.pem"
export LOCALSTACK_HOST_MOUNTS="/home/your_user/custom_ca_bundle.pem:/etc/ssl/certs/custom_ca_bundle.pem"
```
This assumes you have already created the custom_ca_bundle.pem file as described in the main project README, CICA specific Windows WSL setup and confguration instructions.
- Ensure your `local-dev-environment/.env` file is created and contains valid AWS credentials for the `AWS_MOD_PLATFORM_*` variables. You can copy the structure from the `local-dev-environment/.env_template` file.


## Setup

Navigate into the local-dev-environment folder 

```cd cica-review-case-documents-airflow/local-dev-environment```

and run

```docker compose up -d --force-recreate```

You should see 

```
:~/cica-review-case-documents-airflow/local-dev-environment$ docker compose up -d --force-recreate
[+] Running 5/5
 ✔ Network local-dev-environment_default  Created                                             
 ✔ Volume "local-dev-environment_data01"  Created                                         
 ✔ Container opensearch                  Started                                             
 ✔ Container localstack-main             Healthy                                     
 ✔ Container opensearch-dashboards       Started   
```

View the localstack logs

```docker  logs localstack-main -f```

Look for 

```
Waiting for OpenSearch domain to be created...
2025-07-09T13:38:52.761  INFO --- [et.reactor-0] localstack.request.aws     : AWS opensearch.DescribeDomain => 200
        "Processing": false,
OpenSearch domain created.
2025-07-09T13:38:54.510  INFO --- [et.reactor-0] localstack.request.aws     : AWS opensearch.DescribeDomain => 200
DOMAIN_ENDPOINT: case-document-search-domain.eu-west-2.opensearch.localhost.localstack.cloud:4566
Waiting for OpenSearch to be ready...
```
There may be a short delay whilst the domain spins up then you should see


```
OpenSearch is ready! Creating index 'case-documents'...
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100  1168  100    73  100  1095    174   2610 --:--:-- --:--:-- --:--:--  2794
{"acknowledged":true,"shards_acknowledged":true,"index":"case-documents"}LocalStack resources configuration completed.
Ready.

```

The environment is then ready to be used and the domain endpoint should be: 
```case-document-search-domain.eu-west-2.opensearch.localhost.localstack.cloud:4566```


## Docker Desktop 

If Docker Desktop has been installed you can view, pause, stop, run and view logs for the environment and the individual containers from within the Docker Desktop UI (recommended).

## Opensearch Dashboard

Navigate to http://127.0.0.1:5601/ to view the OpenSearch dashboard.

Navigate to [indices](http://127.0.0.1:5601/app/opensearch_index_management_dashboards#/indices?from=0&search=&showDataStreams=false&size=20&sortDirection=desc&sortField=index) to view the newly created index ```case-documents``` index.

Note you may need to index a document before creating an index pattern, to do so follow the instructions within the main [README](../README.md)

Create an [index pattern](https://docs.opensearch.org/latest/dashboards/management/index-patterns/) name it case-documents

you can start keyword searching read the [quickstart](https://docs.opensearch.org/latest/dashboards/quickstart/)


## Integration Helper

Within the integration_helper directory you will find a number of python scripts that can be used for local development testing.
These are largely redundant now and will most likely be removed in an upcoming commit. 

Use the main application [README](../README.md) for instructions on processing a document.

put_doc.py can be used to insert a document into the local opensearch db, it uses an embedding generated by the embedding_generator.py.
search_for_doc.py searches for the inserted document using the same embedding. 

TODO: add to a local integration test suite.

# Evaluation workflow (`testing/run_evaluation`)

The evaluation runner lets you measure how well different search configurations (single type and hybrid combinations) return expected and acceptable chunks for a set of predefined search terms. It reuses the same LocalStack/OpenSearch setup, but for multiple searches consecutively. It queries OpenSearch, scores the returned chunks, and records metrics/outputs for each run. Currently this is for an input of "golden set" of single search terms but a multi-term set is being created. It also looks only at overall precision and recall (and then further precision if desirable and acceptable terms are found in the output). This focuses on the presence of results but will look to include the search scores and rankings in future to look at order of results.

## How it works

1. `testing/search_looper.py` loads search terms from `testing/testing_docs/search_terms.csv`, runs each through `search_client.local_search_client`, and captures the OpenSearch hits.
2. `testing/term_matching.py` checks whether the hits contain the desirable/acceptable terms listed in the CSV, using the methods enabled via the evaluation settings.
3. Metrics (precision, recall) plus raw chunk info are written to timestamped CSVs, and a cumulative log tracks collates the results.

## Running the evaluation

From the `local-dev-environment` directory:

```bash
python -m testing.run_evaluation
```

Requirements:

- LocalStack/OpenSearch running (see setup above).
- `testing/testing_docs/search_terms.csv` populated with the cases you want to evaluate and you also have `TC19_test_all_chunks.csv`in the testing_docs folder.
- Ensure your `.env` file in the project root contains valid AWS credentials and OpenSearch connection details (see main README for details).

## Changing search configuration

All configuration settings can be altered in `testing/evaluation_settings.py`:

- **Boosts & search types:** `KEYWORD_BOOST`, `SEMANTIC_BOOST`, `FUZZY_BOOST`, `WILDCARD_BOOST`, `ANALYSER_BOOST` (set to `0` to disable a mode). The testing looks only at enabled modes.
- **Result volume & filtering:** `K_QUERIES` controls how many results OpenSearch returns, `SCORE_FILTER` drops low-scoring hits before evaluating.
- **Fuzzy parameters:** `FUZZINESS`, `MAX_EXPANSIONS`.
- **Term-matching threshold:** `FUZZY_MATCH_THRESHOLD` governs how lenient fuzzy term checks are.
- **Optional scope filters:** `CASE_REF_FILTER` and `DOCUMENT_ID_FILTER` limit searches to a specific case or document if set. (To be added)

Edit those values, re-run the evaluation command, and the new configuration will be added to the output/log automatically.

## Output locations

- **Per-run CSV:** `local-dev-environment/testing/output/evaluation/<YYYY-MM-DD>/search_type/<timestamp>_relevance_results.csv` contains detailed row-by-row results for each search term.
- **Cumulative log:** `local-dev-environment/testing/output/evaluation/evaluation_log.csv` appends an aggregate summary (precision/recall, chunk match %, etc.) for every run so you can compare configurations over time.

# Using the Hybrid Search Client (`testing/search_client.py`)

This script allows you to perform hybrid (keyword + semantic) search queries against your local OpenSearch instance and export the results to Excel. You can choose for the keyword section to be either direct keyword or to use fuzzy

## Prerequisites

- Ensure your OpenSearch instance is running and accessible.
- Ensure your `.env` file in the project root contains valid AWS credentials and OpenSearch connection details (see main README for details).
- Ensure your `local-dev-environment/.env` file has the same valid AWS credentials see the `local-dev-environment/.env_template` file. 

## Running the Search Client

From the `local-dev-environment` directory:

```bash
python -m testing.search_client
```

This generates an embedding for the hard-coded `SEARCH_TERM`, issues a hybrid OpenSearch query, and writes the filtered hits to Excel.

## Configure it

Results will be written to an Excel file in the `output/hybrid-test-results/<date>/` directory.

## Local Environment Resources

The local environment uses Docker to spin up LocalStack and OpenSearch. The `docker-compose.yml` file orchestrates this setup.

### AWS Resources (via LocalStack)

The following AWS resources are automatically created by the `init-scripts/create-aws-resources.sh` script when you run `docker compose up`. All resource names are configured in the `.env_template` file.

-   **S3 Buckets**:
    -   `local-kta-documents-bucket`: For storing the source PDF documents to be processed.
    -   `document-page-bucket`: For storing the generated PNG images of each document page.
-   **SQS Queue**:
    -   `cica-document-search-queue`: A queue for receiving messages to trigger the ingestion pipeline.

### Automatic Sample Document Provisioning

When you start the local environment, a sample PDF is automatically downloaded from a real AWS S3 bucket (defined by `SRC_S3_BUCKET` and `SRC_S3_KEY` in your `.env` file) and uploaded to the `local-kta-documents-bucket` in LocalStack. This allows you to test the full pipeline without any manual setup.

- Update `SEARCH_TERM` near the top of `testing/search_client.py` for ad-hoc queries.
- All boosts, thresholds, k-values, filters, and fuzzy parameters are shared with the evaluation workflow via `testing/evaluation_settings.py`, so changes there affect both this script and `python -m testing.run_evaluation`.

## Output

Excel files are written to `testing/output/hybrid-test-results/<YYYY-MM-DD>/<timestamp>_<term>_search_results.xlsx` with the active configuration stamped in the header rows.