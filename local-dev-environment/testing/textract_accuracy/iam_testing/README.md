# IAM Handwriting OCR Accuracy Testing

> **Status: In Development** — This testing module is under active development and not yet production-ready.

Evaluates AWS Textract OCR accuracy on handwritten text using the [IAM Handwriting Database](https://fki.tic.heia-fr.ch/databases/iam-handwriting-database), with optional LLM augmentation to improve results.

## Overview

This pipeline:
1. Runs AWS Textract on IAM handwriting form images
2. Separates printed vs handwritten text (Textract provides this classification)
3. Filters dataset-specific headers/footers (e.g., "Sentence Database", form IDs)
4. Calculates WER/CER against ground truth
5. Optionally applies LLM correction via AWS Bedrock to improve accuracy

## Metrics

| Metric | Description |
|--------|-------------|
| **WER** (Word Error Rate) | Fraction of words incorrect (0.0 = perfect, 1.0 = all wrong) |
| **CER** (Character Error Rate) | Fraction of characters incorrect |

## Quick Start

### Prerequisites

1. **AWS Credentials**: Set in `local-dev-environment/settings/dev.yaml`:
   ```yaml
   AWS_ACCESS_KEY_ID: "..."
   AWS_SECRET_ACCESS_KEY: "..."
   AWS_SESSION_TOKEN: "..."  # if using SSO
   AWS_REGION: "us-east-1"
   ```

2. **IAM Dataset**: Download from [IAM Database](https://fki.tic.heia-fr.ch/databases/iam-handwriting-database) and place in:
   ```
   data/
   ├── page_images/          # PNG images (e.g., a01-000u.png)
   ├── xml/                  # Ground truth XML files
   └── ground_truth.jsonl    # Generated by ground_truth_parser.py
   ```

3. **Dependencies**: Ensure `jiwer` is installed:
   ```bash
   uv pip install jiwer
   ```

### Parse Ground Truth (One-Time Setup)

```bash
python -m iam_testing.ground_truth_parser
```
Creates `data/ground_truth.jsonl` with 1539 form records.

### Test Single Form

Validate the pipeline with one form (low API cost):

```bash
python -m iam_testing.runners.single
python -m iam_testing.runners.single --form-id r06-121
python -m iam_testing.runners.single --form-id r06-121 --show-full-text
```

Output: `data/latest_single_test.json`

### Run Baseline Batch

Process all forms through Textract (no LLM):

```bash
python -m iam_testing.runners.batch --limit 10          # Test with 10 forms
python -m iam_testing.runners.batch                     # All 1539 forms
python -m iam_testing.runners.batch --resume            # Resume interrupted run
```

Output:
- `data/ocr_results_<run_id>.jsonl` — Raw OCR output
- `data/score_results_<run_id>.jsonl` — WER/CER scores

### Run LLM Augmentation

Apply LLM correction to baseline results:

```bash
# List available baseline runs
ls data/score_results_*.jsonl

# Augment a baseline run
python -m iam_testing.runners.augment --baseline-run 20260126_140000
python -m iam_testing.runners.augment --baseline-run 20260126_140000 --model nova-pro
python -m iam_testing.runners.augment --baseline-run 20260126_140000 --limit 10
```

Available models:
| Model | Notes |
|-------|-------|
| `nova-micro` | Fastest, cheapest |
| `nova-lite` | Default, good balance |
| `nova-pro` | Best Nova quality |
| `claude-3-haiku` | Requires Bedrock subscription |
| `claude-3-5-sonnet` | Best quality, requires subscription |

Output: `data/augmented_results_<run_id>_<model>.jsonl`

## Package Structure

```
iam_testing/
├── runners/              # CLI entry points
│   ├── single.py         # Single-form test
│   ├── batch.py          # Batch baseline runner
│   └── augment.py        # LLM augmentation runner
├── llm/                  # LLM client layer
│   ├── __init__.py       # Factory: get_llm_client()
│   ├── clients.py        # Nova + Claude implementations
│   ├── prompt.py         # Versioned system prompt
│   └── response.py       # LLMResponse dataclass
├── schemas.py            # OCRResult, WordBlock
├── scoring.py            # ScoreResult, WER/CER calculation
├── textract_client.py    # AWS Textract utilities
├── textract_ocr.py       # OCR processing pipeline
├── iam_filters.py        # Header/footer/signature filters
├── ground_truth_parser.py # IAM XML → JSONL parser
└── config.py             # Settings loader
```

## Output Formats

### score_results (Baseline)
```json
{
  "form_id": "r06-121",
  "wer_handwriting": 0.0789,
  "cer_handwriting": 0.0236,
  "wer_print": 0.0,
  "cer_print": 0.0,
  "gt_handwriting_text": "...",
  "ocr_handwriting_text": "..."
}
```

### augmented_results (LLM)
```json
{
  "form_id": "r06-121",
  "llm_model": "nova-lite",
  "prompt_version": "v1.0_9ca55a78",
  "baseline_wer": 0.0789,
  "augmented_wer": 0.0395,
  "wer_improvement": 0.0394,
  "diff_summary": "'balked' -> 'talked'; 'lund' -> 'lunch'"
}
```

## Cost Estimates

| Operation | Cost per form | 1000 forms |
|-----------|---------------|------------|
| Textract | ~$0.0015 | ~$1.50 |
| Nova Lite | ~$0.0002 | ~$0.20 |
| Claude Haiku | ~$0.0003 | ~$0.30 |

*Estimates based on typical form size. Actual costs may vary.*
